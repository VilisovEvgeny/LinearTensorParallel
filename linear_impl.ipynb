{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa121e1a-926b-4e56-a7a3-3517e7f6cb7a",
   "metadata": {},
   "source": [
    "# init all requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0137a8ab-d4c1-4a32-95ac-c2edc4673f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Параметры\n",
    "lr = 0.3\n",
    "batch_size = 32\n",
    "in_ch = 1024\n",
    "out_ch = 608\n",
    "\n",
    "# Матрицы и вектораы для ручного линейного слоя\n",
    "Y_gt = torch.randn(batch_size, out_ch)\n",
    "\n",
    "X = torch.randn(batch_size, in_ch)\n",
    "W = torch.randn(out_ch, in_ch)\n",
    "b = torch.randn(1, out_ch)\n",
    "\n",
    "# Линейный слой для сравнения\n",
    "linear = nn.Linear(in_ch, out_ch)\n",
    "with torch.no_grad():\n",
    "    linear.weight.copy_(W.clone())\n",
    "    linear.bias.copy_(b.squeeze().clone())\n",
    "\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=lr)\n",
    "\n",
    "X_t = X.clone()\n",
    "Y_pred = linear(X_t)\n",
    "\n",
    "loss = ((Y_pred - Y_gt) ** 2).mean()\n",
    "loss.backward()\n",
    "\n",
    "# Градиенты для проверки\n",
    "dW_torch = linear.weight.grad.clone()\n",
    "db_torch = linear.bias.grad.clone()\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85410d12-0f89-452a-aeca-b084ea40b832",
   "metadata": {},
   "source": [
    "# regular linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff9aa81-2247-460f-90fa-3d197d8e5fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dX:  torch.Size([32, 1024]) , dW:  torch.Size([1024, 608]) , db:  torch.Size([608])\n",
      "\n",
      "Is weights matching nn.Linear: True\n",
      "Is bias matching nn.Linear:    True\n",
      "Is grad W matching nn.Linear:  True\n",
      "Is grad b matching nn.Linear:  True\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Обычная реализация прямого и обратного прохода линейного слоя, и корректировки весов:\n",
    "'''\n",
    "\n",
    "Y = X @ W.T + b\n",
    "\n",
    "# По аналогии с тем, как получаются градиенты MSE, \n",
    "# чтобы было совпадение с линейным слоем torch-а\n",
    "dY = 2 * (Y - Y_gt) / (X.shape[0] * out_ch) \n",
    "\n",
    "# в текущем пайплайне вычислять dX особо смысла нет,\n",
    "# т.к. блок один и градиент передавать не надо\n",
    "dX = dY @ W\n",
    "dW = X.T @ dY\n",
    "db = dY.sum(dim=0)\n",
    "\n",
    "print(\"dX: \", dX.shape, \", dW: \", dW.shape, \", db: \", db.shape)\n",
    "\n",
    "W_manual = W - lr * dW.T\n",
    "b_manual = b - lr * db.unsqueeze(dim=0)\n",
    "\n",
    "# ПРоверям, что вручную написанный линейный слой \n",
    "# ведет себя идентично torch.nn.Linear-у\n",
    "print(f\"\\nIs weights matching nn.Linear: {torch.allclose(W_manual, linear.weight, atol=1e-6)}\")\n",
    "print(f\"Is bias matching nn.Linear:    {torch.allclose(b_manual, linear.bias.unsqueeze(0), atol=1e-6)}\")\n",
    "print(f\"Is grad W matching nn.Linear:  {torch.allclose(dW.T, dW_torch, atol=1e-6)}\")\n",
    "print(f\"Is grad b matching nn.Linear:  {torch.allclose(db.squeeze(), db_torch, atol=1e-6)}\")\n",
    "\n",
    "print(\"\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b83f04-10fa-45b3-b774-6cee296b6db0",
   "metadata": {},
   "source": [
    "# column-wise tensor parallel\n",
    "\n",
    "\n",
    "![column_wise_tp_scheme.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_diagram2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7020ad55-23e6-4293-a27a-115f89febdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes:\n",
      "W_gpu1:  torch.Size([304, 1024]) \n",
      "W_gpu2:  torch.Size([304, 1024]) \n",
      "b_gpu1:  torch.Size([1, 304]) \n",
      "b_gpu2:  torch.Size([1, 304]) \n",
      "X_gpu1:  torch.Size([32, 1024]) \n",
      "X_gpu2:  torch.Size([32, 1024])\n",
      "\n",
      "W_comb:  torch.Size([608, 1024]) \n",
      "b_comb:  torch.Size([1, 608]) \n",
      "dW_comb:  torch.Size([1024, 608]) \n",
      "db_comb:  torch.Size([608])\n",
      "\n",
      "Is weights matching nn.Linear: True\n",
      "Is bias matching nn.Linear:    True\n",
      "Is grad W matching nn.Linear:  True\n",
      "Is grad b matching nn.Linear:  True\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Идейная реализация column-wise tensor_parallel:\n",
    "'''\n",
    "\n",
    "\n",
    "# Разбиваем на два устройства (на shard-ы)\n",
    "part_size = W.shape[0] // 2\n",
    "\n",
    "W_gpu1 = W[:part_size, :].clone()\n",
    "W_gpu2 = W[part_size:, :].clone()\n",
    "b_gpu1 = b[:, :part_size].clone()\n",
    "b_gpu2 = b[:, part_size:].clone()\n",
    "\n",
    "# broadcast\n",
    "X_gpu1 = X.clone()\n",
    "X_gpu2 = X.clone()\n",
    "\n",
    "print(\"Input shapes:\")\n",
    "print(\"W_gpu1: \", W_gpu1.shape, \n",
    "      \"\\nW_gpu2: \", W_gpu2.shape, \n",
    "      \"\\nb_gpu1: \", b_gpu1.shape, \n",
    "      \"\\nb_gpu2: \", b_gpu2.shape, \n",
    "      \"\\nX_gpu1: \", X_gpu1.shape, \n",
    "      \"\\nX_gpu2: \", X_gpu2.shape,\n",
    "     )\n",
    "\n",
    "\n",
    "# ------------------------ #\n",
    "# Основной пайплайн прямого и обратного прохода, внутри которого \n",
    "# не должна создаваться полная матрица весов W\n",
    "\n",
    "# ----FORWARD----\n",
    "# считаем части предиктов для каждой части (на каждом device-е)\n",
    "Y_gpu1 = X_gpu1 @ W_gpu1.T + b_gpu1\n",
    "Y_gpu2 = X_gpu2 @ W_gpu2.T + b_gpu2\n",
    "\n",
    "# all_gather\n",
    "Y = torch.cat((Y_gpu1, Y_gpu2), 1)\n",
    "\n",
    "\n",
    "# ----BACKWARD----\n",
    "dY = 2 * (Y - Y_gt) / (X.shape[0] * out_ch) \n",
    "\n",
    "# Cчитаем градиенты для shard-а на каждой gpu\n",
    "dX_gpu1 = dY[:, :part_size] @ W_gpu1\n",
    "dX_gpu2 = dY[:, part_size:] @ W_gpu2\n",
    "# в текущем пайплайне вычислять dX особо смысла нет,\n",
    "# т.к. блок один и градиент передавать не надо\n",
    "dX = dX_gpu1 + dX_gpu2\n",
    "\n",
    "dW_gpu1 = X_gpu1.T @ dY[:, :part_size]\n",
    "dW_gpu2 = X_gpu2.T @ dY[:, part_size:]\n",
    "db_gpu1 = dY[:, :part_size].sum(dim=0)\n",
    "db_gpu2 = dY[:, part_size:].sum(dim=0)\n",
    "\n",
    "# Обновление весов\n",
    "W_gpu1 -= lr * dW_gpu1.T\n",
    "W_gpu2 -= lr * dW_gpu2.T\n",
    "b_gpu1 -= lr * db_gpu1.unsqueeze(dim=0)\n",
    "b_gpu2 -= lr * db_gpu2.unsqueeze(dim=0)\n",
    "# ------------------------ #\n",
    "\n",
    "\n",
    "# Объединяем матрица и градиенты для сравнения\n",
    "W_comb = torch.cat((W_gpu1, W_gpu2), 0)\n",
    "b_comb = torch.cat((b_gpu1, b_gpu2), 1)\n",
    "dW_comb = torch.cat((dW_gpu1, dW_gpu2), 1)\n",
    "db_comb = torch.cat((db_gpu1, db_gpu2), 0)\n",
    "\n",
    "print(\"\\nW_comb: \", W_comb.shape, \n",
    "      \"\\nb_comb: \", b_comb.shape, \n",
    "      \"\\ndW_comb: \", dW_comb.shape, \n",
    "      \"\\ndb_comb: \", db_comb.shape,\n",
    "     )\n",
    "\n",
    "# Проверяем, идентичен ли пайплайн nn.Linear-у\n",
    "print(f\"\\nIs weights matching nn.Linear: {torch.allclose(W_comb, linear.weight, atol=1e-6)}\")\n",
    "print(f\"Is bias matching nn.Linear:    {torch.allclose(b_comb, linear.bias.unsqueeze(0), atol=1e-6)}\")\n",
    "print(f\"Is grad W matching nn.Linear:  {torch.allclose(dW_comb.T, dW_torch, atol=1e-6)}\")\n",
    "print(f\"Is grad b matching nn.Linear:  {torch.allclose(db_comb, db_torch, atol=1e-6)}\")\n",
    "\n",
    "print(\"\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a905aa-f380-4451-8cb0-10fd07e0a981",
   "metadata": {},
   "source": [
    "# row-wise tensor parallel\n",
    "\n",
    "\n",
    "![row_wise_tp_scheme.png](https://nanotron-ultrascale-playbook.static.hf.space/assets/images/tp_diagram3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "970c5725-e1f0-437a-a398-d94b09973042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes:\n",
      "W_gpu1:  torch.Size([608, 512]) \n",
      "W_gpu2:  torch.Size([608, 512]) \n",
      "b_gpu1:  torch.Size([1, 608]) \n",
      "b_gpu2:  torch.Size([1, 608]) \n",
      "X_gpu1:  torch.Size([32, 512]) \n",
      "X_gpu2:  torch.Size([32, 512])\n",
      "\n",
      "W_comb:  torch.Size([608, 1024]) \n",
      "b_comb:  torch.Size([1, 608]) \n",
      "dW_comb:  torch.Size([608, 1024]) \n",
      "db_comb:  torch.Size([608]) \n",
      "dX:  torch.Size([32, 1024])\n",
      "\n",
      "Is weights matching nn.Linear: True\n",
      "Is bias matching nn.Linear:    True\n",
      "Is grad W matching nn.Linear:  True (max diff = 4.0978193283081055e-08)\n",
      "Is grad b matching nn.Linear:  True\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Идейная реализация row-wise tensor_parallel:\n",
    "'''\n",
    "\n",
    "\n",
    "# Разбиваем на два устройства (на shard-ы)\n",
    "part_size = W.shape[1] // 2\n",
    "\n",
    "W_gpu1 = W[:, :part_size].clone()\n",
    "W_gpu2 = W[:, part_size:].clone()\n",
    "\n",
    "b_gpu1 = b.clone()\n",
    "b_gpu2 = b.clone()\n",
    "\n",
    "# scatter\n",
    "X_gpu1 = X[:, :part_size]\n",
    "X_gpu2 = X[:, part_size:]\n",
    "\n",
    "print(\"Input shapes:\")\n",
    "print(\"W_gpu1: \", W_gpu1.shape, \n",
    "      \"\\nW_gpu2: \", W_gpu2.shape, \n",
    "      \"\\nb_gpu1: \", b_gpu1.shape, \n",
    "      \"\\nb_gpu2: \", b_gpu2.shape, \n",
    "      \"\\nX_gpu1: \", X_gpu1.shape, \n",
    "      \"\\nX_gpu2: \", X_gpu2.shape,\n",
    "     )\n",
    "\n",
    "# ------------------------ #\n",
    "# Основной пайплайн прямого и обратного прохода, внутри которого \n",
    "# не должна создаваться полная матрица весов W\n",
    "\n",
    "# ----FORWARD----\n",
    "# считаем части предиктов для каждой части (на каждом device-е)\n",
    "Y_gpu1 = X_gpu1 @ W_gpu1.T\n",
    "Y_gpu2 = X_gpu2 @ W_gpu2.T\n",
    "\n",
    "Y = Y_gpu1 + Y_gpu2 + b\n",
    "\n",
    "dY = 2 * (Y - Y_gt) / (X.shape[0] * out_ch) \n",
    "\n",
    "dX_gpu1 = dY @ W_gpu1\n",
    "dW_gpu1 = dY.T @ X_gpu1\n",
    "\n",
    "dX_gpu2 = dY @ W_gpu2\n",
    "dW_gpu2 = dY.T @ X_gpu2\n",
    "\n",
    "# в текущем пайплайне вычислять dX особо смысла нет,\n",
    "# т.к. блок один и градиент передавать не надо\n",
    "dX = torch.cat((dX_gpu1, dX_gpu2), 1)\n",
    "db = dY.sum(dim=0)\n",
    "\n",
    "W_gpu1 -= lr * dW_gpu1\n",
    "W_gpu2 -= lr * dW_gpu2\n",
    "\n",
    "b_tp_row = b - lr * db.unsqueeze(dim=0)\n",
    "\n",
    "\n",
    "# Объединяем матрица и градиенты для сравнения\n",
    "W_comb = torch.cat((W_gpu1, W_gpu2), 1)\n",
    "dW_comb = torch.cat((dW_gpu1, dW_gpu2), 1)\n",
    "\n",
    "print(\"\\nW_comb: \", W_comb.shape, \n",
    "      \"\\nb_comb: \", b_tp_row.shape, \n",
    "      \"\\ndW_comb: \", dW_comb.shape, \n",
    "      \"\\ndb_comb: \", db.shape,\n",
    "      \"\\ndX: \", dX.shape,\n",
    "     )\n",
    "\n",
    "# Проверяем, идентичен ли пайплайн nn.Linear-у\n",
    "print(f\"\\nIs weights matching nn.Linear: {torch.allclose(W_comb, linear.weight, atol=1e-6)}\")\n",
    "print(f\"Is bias matching nn.Linear:    {torch.allclose(b_tp_row, linear.bias.unsqueeze(0), atol=1e-6)}\")\n",
    "# Пришлось добавить atol=1e-6\n",
    "# (подробнее описание ниже и в readme)\n",
    "dW_diff = (dW_comb - dW_torch).abs().max()\n",
    "print(f\"Is grad W matching nn.Linear:  {torch.allclose(dW_comb, dW_torch, atol=1e-6)} (max diff = {dW_diff})\")\n",
    "print(f\"Is grad b matching nn.Linear:  {torch.allclose(db, db_torch, atol=1e-6)}\")\n",
    "\n",
    "print(\"\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913c5028-572d-45bc-bb03-5e5a6d7ac13b",
   "metadata": {},
   "source": [
    "# Вывод:\n",
    "\n",
    "Пункты задания выполнены. Пункт о том, что \"Ни в какой момент времени полная матрица ни на одном из устройств  \n",
    "не должна появляться в памяти\" строго выполняются внутри Forward и Backward. Поскольку оценка и инициализация находятся  \n",
    "вне **column-wise** и **row-wise tensor_parallel (TP)**, можно считать условие выполненным.  \n",
    "\n",
    "Реализовать **column-wise TP** было довольно просто, как и реализовать **row-wise TP**.\n",
    "\n",
    "Основные проблемы возникли при сравнении градиентов и весов с таргетами, полученными из **torch.nn.Linear**,  \n",
    "через **torch.allclose()**.  \n",
    "Тогда как итоговый **dW** у ручной реализации и **column-wise TP** были идентичными матрицам из torch.allclose(),  \n",
    "значения **dW** получаемого в ходе работы **row-wise TP** незначительно отличались от целевых, что при использовании  \n",
    "torch.allclose() без atol приводило возникновению False при сравнении.  \n",
    "\n",
    "С увеличением размерности эта проблема проявляется и в остальных двух пайплайнах.\n",
    "\n",
    "После подсчета фактического diff-а между row-wise **dW** и таргетным **dW** было обнаружено несоответствие на **e-08** доли.  \n",
    "Как я понимаю, проблема возникает из-за того, что операции FP32 неассоциативны (описано в статье https://arxiv.org/html/2506.09501v1).  \n",
    "А обнаружил я ее на более низких размерностях именно у **row-wise TP** потому, что в отличе от предыдущих двух способов у row-wise  \n",
    "GEMM разбит на два независимых, это отличие от оригинального nn.Linear приводит к изменению порядка FP суммирования и как следствие к  \n",
    "более выраженному численному расхождению.\n",
    "\n",
    "![row_wise_tp_scheme.png](https://github.com/VilisovEvgeny/LinearTensorParallel/blob/main/images/fp32_tablet.png?raw=true)\n",
    "\n",
    "Поскольку данная проблема является скорее фундаментальной особенностью, пока что, с ней остается только смириться.  \n",
    "Поэтому применяем atol=1e-6 и проходим проверку, игнорируя незначительные отличия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2925d31-a7eb-4969-9023-55ad1722d52c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultral",
   "language": "python",
   "name": "ultral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
